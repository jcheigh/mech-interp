# mech-interp

Mechanistic Interpretability (MI) is a subfield of deep learning focused on reverse engineering neural networks. I've been interested in this field for some time, but now I want to spend time really diving into it. To start off, primarily I will be following [https://www.neelnanda.io/about](Neel Nanda's) [https://www.neelnanda.io/mechanistic-interpretability/getting-started](Getting Started in Transformer MI). For some things, I will include more high level takeaways, whereas certain topics that I believe are particularly important (or I have less experience with) I'll dive deeper into. 

