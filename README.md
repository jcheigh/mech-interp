# mech-interp

Mechanistic Interpretability (MI) is a subfield of deep learning focused on reverse engineering neural networks. I've been interested in this field for some time, but now I want to spend time really diving into it. To start off, primarily I will be following [Neel Nanda's](https://www.neelnanda.io/about) [Getting Started in Transformer MI](https://www.neelnanda.io/mechanistic-interpretability/getting-started). For some things, I will include more high level takeaways, whereas certain topics that I believe are particularly important (or I have less experience with) I'll dive deeper into. 

