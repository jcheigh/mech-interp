{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj1iFX9JJ1/hxcFZ6XIIeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcheigh/mech-interp/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "\n",
        "### Background:\n",
        "\n",
        "The transformer architecture is fundamental to understanding deep learning. Here I have code for a basic GPT-2 style transformer, following [Neel Nanda's GPT-2 From Scratch](https://www.youtube.com/watch?v=dsjUDacBw8o&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2).\n",
        "\n",
        "Most of this is really an unvectorized step by step implementation. Rather than a bunch of matrix multiplications, there are more loops and dictionaries. My opinion is that this is more intuitive, but that this helps me differentiate between the fundamental part of the model (attention as a way to move info from one position to another etc.) from efficient steps (the fancy attention formula you see)."
      ],
      "metadata": {
        "id": "_st1AO_CyR_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Language Modeling?\n",
        "\n",
        "Language models predict the next token. That is, given some history $h$ and a (fixed) vocabulary $V$, a language model $P_\\theta$ models the distribution over $V$, conditioned on $h$. One can use this to generate text by making it *autoregressive*: 1) start with some prompt, 2) sample from the language model, 3) append this to the prompt, 4) repeat until sampling something akin to a <EOS> (end of sentence) token.\n",
        "\n",
        "### Why Language Modeling?\n",
        "\n",
        "Next token prediction seems like a relatively simple task, and on the surface seems there is no way for any model trained in this way to have superhuman performance. However, one shouldn't underestimate this task. There are perhaps linguistic or even philisophical arguments one can give for this, but I think [Ilya's](https://www.youtube.com/watch?v=YEUclZdj_Sc) statement here is well put."
      ],
      "metadata": {
        "id": "GXdYVY54BnRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### code from Nanda\n",
        "%pip install git+https://github.com/neelnanda-io/Easy-Transformer.git@clean-transformer-demo\n",
        "!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "%pip install git+https://gith\"ub.com/neelnanda-io/PySvelte.git\n",
        "%pip install fancy_einsum\n",
        "%pip install einops\n"
      ],
      "metadata": {
        "id": "rJpyd1eyMUwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "from dataclasses import dataclass\n",
        "from easy_transformer import EasyTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
        "import tqdm.auto as tqdm"
      ],
      "metadata": {
        "id": "6S64HlA6Mzgs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1- Understanding Inputs and Outputs\n",
        "\n",
        "Transformers (in our case) are language models. They input tokens and output logits. So, before we deal with an input sequence like \"roses are red, violets are \", we first must create a *vocabulary*, i.e. a finite set of tokens. From there, we can *tokenize* our sequence, converting it from a string to a list of tokens, which map to a vector of token indices. We can then run our model to generate logits, from which case we can use softmax to turn this into a distribution over the vocabulary."
      ],
      "metadata": {
        "id": "plT2QeZ31hQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
      ],
      "metadata": {
        "id": "uX2PdmlxM7YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: sequence -> tokens (tokenize)\n"
      ],
      "metadata": {
        "id": "dqQUMl-u2Q4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = 'hi, my name is justin'         ### 1- input sequence\n",
        "tokens         = gpt2.to_tokens(input_sequence)  ### 2- to batch x pos tensor of tokens indices\n",
        "\n",
        "print(f'Input Sequence: {input_sequence}')\n",
        "print(f'Tokens: {tokens}')\n",
        "print(f'Tokens Shape: {tokens.shape} = batch x position')\n",
        "print(f'Tokens (str): {gpt2.to_str_tokens(tokens)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6h3HD7YL2qE",
        "outputId": "279d47a0-7992-446b-89bd-e5d99d57a8c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: hi, my name is justin\n",
            "Tokens: tensor([[50256,  5303,    11,   616,  1438,   318,   655,   259]])\n",
            "Tokens Shape: torch.Size([1, 8]) = batch x position\n",
            "Tokens (str): ['<|endoftext|>', 'hi', ',', ' my', ' name', ' is', ' just', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the shape of the tokens tensor is batch x position. One can see this more clearly if we input multiple sequences, i.e. process in parallel."
      ],
      "metadata": {
        "id": "j40iZM-pNZE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences  = ['hi my name is justin', 'hi']\n",
        "batch_tkns = gpt2.to_tokens(sequences)\n",
        "\n",
        "print(f'Sequences: {sequences}')\n",
        "print(f'Tokens: {batch_tkns}')\n",
        "print(f'Tokens Shape: {batch_tkns.shape}')\n",
        "print(f\"Tokens for string 'hi' : {gpt2.to_str_tokens(batch_tkns[1])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgPG65xnNNYW",
        "outputId": "56d8fd8d-39fe-443f-a7d1-bb8d4c9e0756"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences: ['hi my name is justin', 'hi']\n",
            "Tokens: tensor([[50256,  5303,   616,  1438,   318,   655,   259],\n",
            "        [50256,  5303, 50256, 50256, 50256, 50256, 50256]])\n",
            "Tokens Shape: torch.Size([2, 7])\n",
            "Tokens for string 'hi' : ['<|endoftext|>', 'hi', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: tokens -> logits (this is the transformer!)"
      ],
      "metadata": {
        "id": "g7z9Dsca2fpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits, cache = gpt2.run_with_cache(tokens) ### 3- run model, return logits\n",
        "print(logits.shape)                         # batch x position x vocab, i.e. for *each* token will be a dstn over vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJiAZmSbgJst",
        "outputId": "0335ce8a-941e-465f-d859-236b3181703b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: logits -> prediction (sample then decode)"
      ],
      "metadata": {
        "id": "ZKojBb312z-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs  = logits.log_softmax(dim=-1)     ### 4- softmax to product prod distribution\n",
        "next_token = log_probs[0,-1].argmax(dim=-1) ### 5- take argmax to produce next token\n",
        "print(f'Next Token- Index: {next_token}, Token: {gpt2.tokenizer.decode(next_token)}')\n",
        "\n",
        "next_tokens = torch.cat([tokens, torch.tensor(next_token, dtype=torch.int64)[None, None]], dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vENtX_sm2zcE",
        "outputId": "e5734dd9-c02b-40ca-a4f4-ab0b061d2c9c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next Token- Index: 11, Token: ,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1c51ab819436>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  next_tokens = torch.cat([tokens, torch.tensor(next_token, dtype=torch.int64)[None, None]], dim=-1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the above basic loop, we can generate using GPT-2."
      ],
      "metadata": {
        "id": "QxvpqBkeleEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(input_sequence, n_gen, model=gpt2):\n",
        "  print(f'Generating {n_gen} tokens, starting with: {input_sequence}')\n",
        "  tokens = model.to_tokens(input_sequence)\n",
        "  for _ in range(n_gen):\n",
        "    sequence   = model.to_str_tokens(tokens)\n",
        "    logits, _  = model.run_with_cache(tokens)\n",
        "    log_probs  = logits.log_softmax(dim=-1)\n",
        "    next_token = log_probs[0,-1].argmax(dim=-1)\n",
        "    print(f\"{''.join(sequence)} {model.tokenizer.decode(next_token)}\")\n",
        "    tokens = torch.cat([tokens, torch.tensor(next_token, dtype=torch.int64)[None, None]], dim=-1)\n",
        "\n",
        "  return model.to_str_tokens(tokens)\n",
        "\n",
        "results = generate(input_sequence, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fht_VCMzlixQ",
        "outputId": "e578e95d-49bb-41d3-c292-8f5907c53b39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 10 tokens, starting with: hi, my name is justin\n",
            "<|endoftext|>hi, my name is justin ,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5b814599abb8>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tokens = torch.cat([tokens, torch.tensor(next_token, dtype=torch.int64)[None, None]], dim=-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|>hi, my name is justin,  i\n",
            "<|endoftext|>hi, my name is justin, i  am\n",
            "<|endoftext|>hi, my name is justin, i am  a\n",
            "<|endoftext|>hi, my name is justin, i am a  student\n",
            "<|endoftext|>hi, my name is justin, i am a student  at\n",
            "<|endoftext|>hi, my name is justin, i am a student at  the\n",
            "<|endoftext|>hi, my name is justin, i am a student at the  university\n",
            "<|endoftext|>hi, my name is justin, i am a student at the university  of\n",
            "<|endoftext|>hi, my name is justin, i am a student at the university of  u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aside on Tokenization\n",
        "\n",
        "We've gone over the high level generation process, but this requires us to have access to a vocabulary. The typical method for this is Byte Pair Encoding (BPE).\n",
        "\n",
        "\n",
        "At a high level, we may use UTF-8 to map each Unicode codepoint to 1-4 bytes. To further enable us to specify a vocab size, we can iteratively merge the most frequent token pair, appending this new pair to our vocabulary. For many reasons this is a highly flawed process."
      ],
      "metadata": {
        "id": "nNxnxuTxmcmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def find_most_common_pair(tokens):\n",
        "    return Counter(zip(tokens, tokens[1:])).most_common(1)[0][0]\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    ### list of ints (ids), replace(pair, idx)\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i +=1\n",
        "    return newids\n",
        "\n",
        "def get_vocab(text, vocab_size):\n",
        "  ### BPE implementation to get vocab using 256 ascii char as initial token set\n",
        "  num_merges = vocab_size - 256\n",
        "  tokens     = text.encode('utf-8')\n",
        "  ids        = list(tokens)\n",
        "  merges     = {}\n",
        "  for i in range(num_merges):\n",
        "    pair = find_most_common_pair(ids)\n",
        "    idx = 256 + i\n",
        "    print(f'merging {pair} into token {idx}')\n",
        "    ids = merge(ids, pair, idx)\n",
        "    merges[pair] = idx\n",
        "  return merges"
      ],
      "metadata": {
        "id": "TKARR8G_pQFl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2- Step by Step Unvectorized Implementation\n",
        "\n",
        "To be clear this is not actually a full implementation, as we skip over things like positional embeddings, layernorm, etc."
      ],
      "metadata": {
        "id": "q-ZE-M3w4GjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more high level discussion of transformer check out Neel Nanda's implementation/notes.\n",
        "\n",
        "### Step 1- Preprocessing (tokenization)\n",
        "\n",
        "1. Preprocessing: an input prompt to a batch x position x d_model tensor, where each token (each batch x position) has associated with it a residual stream that starts with it being purely the embedding.\n",
        "  - Break string into a tensor of tokens (batch = 1 for this analysis)\n",
        "  - Map tokens ('a') to indices ({a:0,...}) to get a tensor of indices\n",
        "  - Use embedding lookup table ({0 : [...]}) to get a tensor of embeddings\n",
        "\n",
        "We'll ignore the BPE stuff and just deal with character level tokenization for now."
      ],
      "metadata": {
        "id": "kRHe5-QHrYjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_ln = lambda n=20 : print(f'=' * n)\n",
        "printT   = lambda matrix : np.round(matrix, 2)\n",
        "\n",
        "### 1- start with input\n",
        "print_ln()\n",
        "input_sequence = 'hello, there.'\n",
        "print(f'Input sequence: {input_sequence}')\n",
        "\n",
        "### 2- standard preprocessing (for us this is really simple!)\n",
        "print_ln()\n",
        "input_sequence = input_sequence.lower()\n",
        "input_sequence = input_sequence.replace('.', ' .')\n",
        "input_sequence = input_sequence.replace(',', ' ,')\n",
        "print(f'Processed input sequence: {input_sequence}')\n",
        "\n",
        "### 3- define vocab and tokenize\n",
        "print_ln()\n",
        "vocab = list('abcdefghijklmnopqrstuvwxyz., ')\n",
        "vocab.append('<EOS>') ### either eos/sos\n",
        "indices = list(range(len(vocab)))\n",
        "tkn_to_idx = {t:i for i, t in enumerate(vocab)}\n",
        "idx_to_tkn = {i:t for i, t in enumerate(vocab)}\n",
        "print(f'Vocabulary: {vocab}')\n",
        "\n",
        "def tokenize(sequence):\n",
        "  tokens = []\n",
        "  for t in sequence:\n",
        "    if t not in tkn_to_idx:\n",
        "      raise Exception(f'Token {t} not in vocab idiot (jk...)')\n",
        "\n",
        "    tokens.append(tkn_to_idx[t])\n",
        "  return tokens + [tkn_to_idx['<EOS>']]\n",
        "\n",
        "def to_str(tokens):\n",
        "  return ''.join([idx_to_tkn[i] for i in tokens])\n",
        "\n",
        "tokens   = tokenize(input_sequence)\n",
        "sequence = to_str(tokens)\n",
        "print_ln()\n",
        "print(f'Tokens: {tokens}')\n",
        "print(f'Num Tokens: {len(tokens)}')\n",
        "assert sequence == input_sequence + '<EOS>'\n",
        "\n",
        "d_vocab  = len(vocab)\n",
        "d_model  = 6\n",
        "position = len(tokens)\n",
        "\n",
        "print_ln()\n",
        "print(f'd_vocab: {d_vocab}')\n",
        "print(f'd_model: {d_model}')\n",
        "print(f'position: {position}')\n",
        "\n",
        "embedding_map = {i : np.random.standard_normal(d_model) for i in indices}\n",
        "\n",
        "for i in range(4):\n",
        "  print_ln(20)\n",
        "  print(f'Token {i}: {idx_to_tkn[i]}')\n",
        "  print(f'Embedding: {embedding_map[i][:5]}')\n",
        "\n",
        "### 4- embed using lookup table\n",
        "def embed(sequence):\n",
        "  tokens = tokenize(sequence)\n",
        "  return np.array([embedding_map[i] for i in tokens]), tokens\n",
        "\n",
        "res_stream_tensor, _ = embed(input_sequence)\n",
        "print_ln()\n",
        "print(f'Transformer Input:\\n {printT(res_stream_tensor)}')\n",
        "print(f'Transformer Input Shape: {res_stream_tensor.shape} (position x d_model)')\n",
        "assert res_stream_tensor.shape == (len(tokens), d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnZEi1BNM83I",
        "outputId": "e1c90952-78c3-416a-8232-f0de7dc80d5c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "Input sequence: hello, there.\n",
            "====================\n",
            "Processed input sequence: hello , there .\n",
            "====================\n",
            "Vocabulary: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '.', ',', ' ', '<EOS>']\n",
            "====================\n",
            "Tokens: [7, 4, 11, 11, 14, 28, 27, 28, 19, 7, 4, 17, 4, 28, 26, 29]\n",
            "Num Tokens: 16\n",
            "====================\n",
            "d_vocab: 30\n",
            "d_model: 6\n",
            "position: 16\n",
            "====================\n",
            "Token 0: a\n",
            "Embedding: [-1.83168225 -1.05130991  1.49743168  1.85770234 -0.10343122]\n",
            "====================\n",
            "Token 1: b\n",
            "Embedding: [ 2.0958435   1.59444239  0.67894699 -0.81240235 -0.04902934]\n",
            "====================\n",
            "Token 2: c\n",
            "Embedding: [ 0.3308513   1.45100144  0.87941704 -1.07737608  1.37649645]\n",
            "====================\n",
            "Token 3: d\n",
            "Embedding: [ 0.6870658   1.46654086 -1.11158025 -0.03582558 -0.53145455]\n",
            "====================\n",
            "Transformer Input:\n",
            " [[-0.3   1.31  0.32  0.19 -1.27  0.29]\n",
            " [ 0.35  2.51 -1.84 -0.03  0.64  0.12]\n",
            " [ 1.26  1.15 -0.97  1.01  0.34 -0.23]\n",
            " [ 1.26  1.15 -0.97  1.01  0.34 -0.23]\n",
            " [-0.18  0.4   0.69  0.73 -0.99 -0.28]\n",
            " [-0.57  0.33  0.93 -0.22  1.07  1.45]\n",
            " [ 0.53  0.84  1.22  1.06  0.87 -0.32]\n",
            " [-0.57  0.33  0.93 -0.22  1.07  1.45]\n",
            " [-0.99 -0.04 -0.81 -1.11  0.26  0.21]\n",
            " [-0.3   1.31  0.32  0.19 -1.27  0.29]\n",
            " [ 0.35  2.51 -1.84 -0.03  0.64  0.12]\n",
            " [ 0.6  -0.63 -0.24  0.74  0.42  0.27]\n",
            " [ 0.35  2.51 -1.84 -0.03  0.64  0.12]\n",
            " [-0.57  0.33  0.93 -0.22  1.07  1.45]\n",
            " [ 0.6  -0.85  2.49  0.26  1.28 -0.15]\n",
            " [-0.92 -0.84 -0.14  0.31 -0.52  1.35]]\n",
            "Transformer Input Shape: (16, 6) (position x d_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2- Transformer Block\n",
        "\n",
        "At this point, we have a residual stream (some sort of global memory) for each token in the sequence. However, this residual stream is initialized to simply be the embedding, i.e. no information about the past context. We therefore need to\n",
        "1. Move information from each prior token to the current token (attention)\n",
        "2. Perform computation on this aggregate of information (MLP)\n",
        "\n",
        "Let's dive a bit deeper into attention\n",
        "\n",
        "### Attention\n",
        "\n",
        "Here are basically Neel's notes. Attention moves information from prior positions in the sequence to the current token.\n",
        "- We do this for each token in parallel with the same parameters, with the only difference being we look backwards only to avoid cheating\n",
        "- This is the only part of the transformer that moves info between positions\n",
        "- We do this with $n_{\\text{heads}}$ heads, each with their own parameter, attention pattern (will discuss later), and own way of copying information from src/past token to destination/current token\n",
        "  - heads act independently and additively, we add their outputs back into the stream\n",
        "- Each head:\n",
        "  - Produces an attention pattern for each destination token, that is a probability distribution of prior source tokens (including the current one), weighing how much information to copy\n",
        "    - Does this for each pair of tokens\n",
        "    - Copies info in the same way from each source token\n",
        "      - Note the info we copy depends on the source token's residual stream, which can be more than simply the info of what text token is at the source token's position\n",
        "    - Note which source token to copy info from (query/key dot product) is distinct from how to copy that info (value mlp)\n",
        "    - $d_\\text{head} = \\frac{d_{\\text{model}}}{n_{\\text{heads}}}$\n",
        "\n",
        "### Mathematically\n",
        "\n",
        "Input: X = residual stream tensor with shape batch x position x d_model\n",
        "\n",
        "Define $W_Q, W_K$ with shape n_heads x d_model x d_head, then multiply to get query tensor $Q$, key tensor $K$, where both are shape batch x position x n_heads x d_head. Here each token has n_heads, each of which has a d_head vector. The query vector is asking for information, the key vector is telling what info the residual stream of that token contains.\n",
        "\n",
        "We then run essentially the following loop: for each batch, for each head, for each query vector, we compute the dot product of that query vector with each key vector, and this attention score is aggregated in a matrix. So we have a batch x n_head x position x position, where the position x position matrix is a query_pos x key_pos matrix, where the (i,j)th entry is the similarity between the ith token's query and the jth token's key.\n",
        "\n",
        "To make this causal, the ith token should only be able to query token j with $j \\le i$, therefore we will mask out anything where $j > i$. We want to make this into a probability distribution, so we apply softmax, which gives us the attention pattern (some normalization here also).\n",
        "\n",
        "We then get a value tensor $V$ with shape batch, position, n_heads, d_head (the position here is key_position) and multiply along the key_position dimension. This means that, for each (destination) token there is an attention pattern over source tokens, as well as a value vector indicating, and the dot product of these is basically the new info to copy, giving us a batch x query_pos x n_heads x d_head tensor.\n",
        "\n",
        "We then apply a linear map to get a batch x position x n_head x d_model tensor and sum over all heads to get our result."
      ],
      "metadata": {
        "id": "8JHwoHz9pamv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### map tkn to residual stream\n",
        "res_stream_map = {tkn : res_stream_tensor[i] for i, tkn in enumerate(tokens)}"
      ],
      "metadata": {
        "id": "Onx3Ce-S8_l_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkn = tokens[2]\n",
        "print(f'Token {tkn} = {idx_to_tkn[tkn]}')\n",
        "residual_stream = res_stream_map[tkn]\n",
        "print(f'Initial Residual Stream: {residual_stream}')\n",
        "\n",
        "### residual stream starts as embedding\n",
        "assert np.all(residual_stream == embedding_map[tkn])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p08cI5RA-v5g",
        "outputId": "c889e216-bb20-4092-b5df-7e507362f8a3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 11 = l\n",
            "Initial Residual Stream: [ 1.25923324  1.14644087 -0.97337884  1.00713334  0.33614496 -0.22747626]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_heads = 2\n",
        "d_head  = int(d_model / n_heads)\n",
        "print(f'n_heads: {n_heads}')\n",
        "print(f'd_head: {d_head}')\n",
        "\n",
        "### 1- get query, key vector for each token\n",
        "query_vec_map = {(tkn, head) : np.random.standard_normal(d_head) for tkn in tokens for head in range(n_heads)}\n",
        "key_vec_map   = {(tkn, head) : np.random.standard_normal(d_head) for tkn in tokens for head in range(n_heads)}\n",
        "print_ln()\n",
        "head = 1\n",
        "print(f'Query Vector for Token {tkn}, Head {head}: {query_vec_map[(tkn, head)]}')\n",
        "print(f'Key Vector for Token {tkn}, Head {head}: {key_vec_map[(tkn, head)]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HNUjInt_UHs",
        "outputId": "6730d0d9-5e96-4a01-a58e-32a02b9687a3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_heads: 2\n",
            "d_head: 3\n",
            "====================\n",
            "Query Vector for Token 11, Head 1: [-2.31614386  0.8580142  -0.82819627]\n",
            "Key Vector for Token 11, Head 1: [ 1.17171791 -1.17617334  0.4875603 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 2- get attn scores- for each head, query_pos x key_pos matrix, where entry (i,j) = dot(query(i), key(j))\n",
        "### or, for each head, for each (destination) token, there is a vector of scores with each position\n",
        "### then, scale by 1/sqrt(d_head), and mask where j > i\n",
        "attn_score_map = {\n",
        "    head : np.zeros((position, position))\n",
        "    for head in range(n_heads)\n",
        "    }\n",
        "\n",
        "print_rng = 2\n",
        "\n",
        "print_ln(50)\n",
        "print(f'Computing Attention Scores')\n",
        "\n",
        "for head in range(n_heads):\n",
        "  if head == 0:\n",
        "    print_ln(40)\n",
        "    print(f'Head {head}')\n",
        "  for i, dest_tkn in enumerate(tokens):\n",
        "    query = query_vec_map[(dest_tkn, head)]\n",
        "    if head == 0 and i < print_rng:\n",
        "      print(f'  ===========================')\n",
        "      print(f'  Destination Token {dest_tkn}')\n",
        "      print(f'  Query Vector: {printT(query)}')\n",
        "\n",
        "    for j, src_tkn in enumerate(tokens):\n",
        "\n",
        "      key = key_vec_map[(src_tkn, head)]\n",
        "      if head == 0 and max(i,j) < print_rng:\n",
        "        print(f'    ================')\n",
        "        print(f'    Source Token {src_tkn}')\n",
        "        print(f'    Key Vector: {printT(key)}')\n",
        "\n",
        "      attn_score           = np.dot(query, key)\n",
        "      ### mask out with j > i and normalize\n",
        "      processed_attn_score = attn_score / np.sqrt(d_head) if j <= i else -1e8\n",
        "\n",
        "      if head == 0 and max(i,j) < print_rng:\n",
        "        print(f'    ================')\n",
        "        print(f'    Attention Score for between Dest {dest_tkn}, Src {src_tkn}: {round(attn_score,2)}')\n",
        "        print(f'    Processed Attention Score: {round(processed_attn_score, 2)}')\n",
        "\n",
        "      attn_score_map[head][i][j] = processed_attn_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMwCUCIUL2W3",
        "outputId": "e68205a4-ca05-4fa8-c678-fe16d190a91d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Computing Attention Scores\n",
            "========================================\n",
            "Head 0\n",
            "  ===========================\n",
            "  Destination Token 7\n",
            "  Query Vector: [-0.81  0.82  0.23]\n",
            "    ================\n",
            "    Source Token 7\n",
            "    Key Vector: [-0.72 -0.2  -0.45]\n",
            "    ================\n",
            "    Attention Score for between Dest 7, Src 7: 0.33\n",
            "    Processed Attention Score: 0.19\n",
            "    ================\n",
            "    Source Token 4\n",
            "    Key Vector: [ 0.95  0.24 -1.53]\n",
            "    ================\n",
            "    Attention Score for between Dest 7, Src 4: -0.92\n",
            "    Processed Attention Score: -100000000.0\n",
            "  ===========================\n",
            "  Destination Token 4\n",
            "  Query Vector: [-0.15 -0.33 -1.04]\n",
            "    ================\n",
            "    Source Token 7\n",
            "    Key Vector: [-0.72 -0.2  -0.45]\n",
            "    ================\n",
            "    Attention Score for between Dest 4, Src 7: 0.64\n",
            "    Processed Attention Score: 0.37\n",
            "    ================\n",
            "    Source Token 4\n",
            "    Key Vector: [ 0.95  0.24 -1.53]\n",
            "    ================\n",
            "    Attention Score for between Dest 4, Src 4: 1.38\n",
            "    Processed Attention Score: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for head in range(n_heads):\n",
        "  print_ln()\n",
        "  attn_scores = attn_score_map[head]\n",
        "  if_mask     = (attn_scores + 1e8) < 0.01\n",
        "  to_print    = np.where(if_mask, \"___\", np.round(attn_scores, 2))[:7,:7]\n",
        "  print(f'Attention Score Matrix for Head {head}: \\n{to_print}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxdMmckRApol",
        "outputId": "b70d6bcd-a39b-4f89-fa21-b78037610a67"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "Attention Score Matrix for Head 0: \n",
            "[['0.19' '___' '___' '___' '___' '___' '___']\n",
            " ['0.37' '0.79' '___' '___' '___' '___' '___']\n",
            " ['-0.02' '1.48' '0.56' '___' '___' '___' '___']\n",
            " ['-0.02' '1.48' '0.56' '0.56' '___' '___' '___']\n",
            " ['0.62' '-1.79' '-0.24' '-0.24' '-1.43' '___' '___']\n",
            " ['-0.08' '-0.11' '-0.17' '-0.17' '-0.05' '0.11' '___']\n",
            " ['1.0' '1.36' '0.85' '0.85' '-0.47' '1.23' '-0.8']]\n",
            "====================\n",
            "Attention Score Matrix for Head 1: \n",
            "[['-0.17' '___' '___' '___' '___' '___' '___']\n",
            " ['0.6' '0.16' '___' '___' '___' '___' '___']\n",
            " ['1.16' '0.33' '-2.38' '___' '___' '___' '___']\n",
            " ['1.16' '0.33' '-2.38' '-2.38' '___' '___' '___']\n",
            " ['0.08' '-0.08' '0.18' '0.18' '-0.11' '___' '___']\n",
            " ['-0.54' '-0.09' '0.37' '0.37' '0.29' '-0.3' '___']\n",
            " ['0.1' '0.08' '-1.26' '-1.26' '2.01' '-1.62' '0.48']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 3- apply softmax row-wise to each attn_score matrix to get attn pattern\n",
        "softmax = lambda vec : np.exp(vec) / np.sum(np.exp(vec))\n",
        "attn_pattern_map = {\n",
        "    head : np.zeros((position, position))\n",
        "    for head in range(n_heads)\n",
        "    }\n",
        "\n",
        "for head in range(n_heads):\n",
        "  attn_score_mat = attn_score_map[head]\n",
        "  for i, dest_tkn in enumerate(tokens):\n",
        "    attn_score_vec   = attn_score_mat[i]\n",
        "    attn_pattern_vec = softmax(attn_score_vec)\n",
        "    attn_pattern_map[head][i] = attn_pattern_vec\n",
        "\n",
        "print_ln(50)\n",
        "for head in range(n_heads):\n",
        "  print_ln()\n",
        "  attn_pattern = attn_pattern_map[head]\n",
        "  print(f'Attention Pattern Matrix for Head {head}: \\n{printT(attn_pattern[:7, :7])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws6OWmRHJBV8",
        "outputId": "17980d47-f03b-4a71-e9fc-fb1884d0412e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "====================\n",
            "Attention Pattern Matrix for Head 0: \n",
            "[[1.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.4  0.6  0.   0.   0.   0.   0.  ]\n",
            " [0.14 0.62 0.25 0.   0.   0.   0.  ]\n",
            " [0.11 0.5  0.2  0.2  0.   0.   0.  ]\n",
            " [0.48 0.04 0.2  0.2  0.06 0.   0.  ]\n",
            " [0.17 0.16 0.15 0.15 0.17 0.2  0.  ]\n",
            " [0.17 0.25 0.15 0.15 0.04 0.22 0.03]]\n",
            "====================\n",
            "Attention Pattern Matrix for Head 1: \n",
            "[[1.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.61 0.39 0.   0.   0.   0.   0.  ]\n",
            " [0.68 0.3  0.02 0.   0.   0.   0.  ]\n",
            " [0.67 0.29 0.02 0.02 0.   0.   0.  ]\n",
            " [0.2  0.17 0.23 0.23 0.17 0.   0.  ]\n",
            " [0.09 0.14 0.22 0.22 0.21 0.11 0.  ]\n",
            " [0.09 0.09 0.02 0.02 0.62 0.02 0.13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 4- get value vector for each token (specifically each src token)\n",
        "### attn_pattern.shape = query_pos x key_pos, value_mat.shape = key_pos x d_head, z = query_pos x d_head\n",
        "value_map = {\n",
        "    head : np.random.standard_normal((position, d_head))\n",
        "    for head in range(n_heads)\n",
        "    }\n",
        "z = {\n",
        "    head : np.matmul(attn_pattern_map[head], value_map[head])\n",
        "    for head in range(n_heads)\n",
        "    }\n",
        "\n",
        "print_ln(50)\n",
        "head = 0\n",
        "print(f'Z matrix for Head {head}: \\n{printT(z[head])}, shape: {z[head].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yQ1GHohLrZa",
        "outputId": "88ef5068-8af9-425f-b69b-e02b81c03873"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Z matrix for Head 0: \n",
            "[[-0.56 -0.13  1.67]\n",
            " [-0.79  0.92  0.47]\n",
            " [-0.33  0.63  0.18]\n",
            " [-0.41  0.23  0.18]\n",
            " [-0.13 -0.62  0.87]\n",
            " [-0.   -0.35  0.2 ]\n",
            " [-0.19 -0.1   0.44]\n",
            " [ 0.2  -0.3   0.24]\n",
            " [ 0.21 -0.45 -0.03]\n",
            " [ 0.18 -0.48  0.24]\n",
            " [ 0.09 -0.06  0.29]\n",
            " [ 0.31 -0.15  0.22]\n",
            " [ 0.29 -0.18  0.23]\n",
            " [ 0.19 -0.24  0.43]\n",
            " [ 0.19 -0.37  0.29]\n",
            " [ 0.11 -0.16  0.31]], shape: (16, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 5- take z (query_pos x d_head) and apply W_O (d_head, d_model) to get result (pos x d_model)\n",
        "W_O = {head : np.random.standard_normal((d_head, d_model)) for head in range(n_heads)}\n",
        "result = {\n",
        "    head : np.matmul(z[head], W_O[head])\n",
        "    for head in range(n_heads)\n",
        "    }\n",
        "\n",
        "for head in range(n_heads):\n",
        "  print_ln()\n",
        "  print(f'Attention Result for Head {head}: \\n{printT(result[head])}')\n",
        "  assert result[head].shape == (position, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwJIJTg4OclK",
        "outputId": "876c4ef1-35af-4eff-e4a9-18f908e4fe68"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "Attention Result for Head 0: \n",
            "[[ 0.34  1.17 -0.7  -2.62 -0.81  0.43]\n",
            " [ 1.43  0.98 -2.55 -0.39 -2.28 -0.46]\n",
            " [ 0.62  0.52 -1.41  0.01 -1.6  -0.35]\n",
            " [ 0.77  0.36 -0.99 -0.27 -0.44 -0.09]\n",
            " [-0.17  0.29  0.66 -1.63  0.94  0.57]\n",
            " [-0.1  -0.04  0.48 -0.49  0.75  0.26]\n",
            " [ 0.19  0.3  -0.18 -0.75 -0.    0.16]\n",
            " [-0.53 -0.05  0.74 -0.45  0.4   0.23]\n",
            " [-0.43 -0.29  0.95 -0.16  1.06  0.27]\n",
            " [-0.49 -0.13  0.95 -0.56  0.92  0.35]\n",
            " [-0.34  0.13  0.24 -0.41 -0.21  0.09]\n",
            " [-0.74 -0.03  0.71 -0.29 -0.08  0.13]\n",
            " [-0.72 -0.03  0.73 -0.33  0.02  0.15]\n",
            " [-0.61  0.09  0.66 -0.68  0.08  0.23]\n",
            " [-0.54 -0.05  0.83 -0.56  0.56  0.29]\n",
            " [-0.37  0.08  0.41 -0.49  0.05  0.16]]\n",
            "====================\n",
            "Attention Result for Head 1: \n",
            "[[-3.16  1.85  3.33 -0.78  1.46 -0.3 ]\n",
            " [-0.22  0.04  0.96  0.14  0.67 -0.74]\n",
            " [-0.88  0.46  1.48 -0.08  0.84 -0.62]\n",
            " [-0.81  0.42  1.42 -0.06  0.81 -0.62]\n",
            " [ 1.06 -0.55 -0.41  0.27  0.09 -0.46]\n",
            " [ 1.24 -0.72 -0.66  0.38 -0.05 -0.45]\n",
            " [ 1.89 -1.23 -0.95  0.75 -0.06 -0.85]\n",
            " [ 0.66 -0.47 -0.28  0.31  0.01 -0.37]\n",
            " [-0.2  -0.17  0.64  0.36  0.39 -0.66]\n",
            " [-0.25 -0.1   0.23  0.25  0.05 -0.23]\n",
            " [-0.72 -0.15  1.45  0.62  0.79 -1.2 ]\n",
            " [ 0.7  -0.81 -1.15  0.62 -0.72  0.06]\n",
            " [-0.26 -0.41  0.77  0.7   0.42 -0.98]\n",
            " [ 0.21 -0.34  0.13  0.37  0.14 -0.49]\n",
            " [ 0.36 -0.46 -0.23  0.42 -0.08 -0.33]\n",
            " [ 1.58 -1.25 -1.74  0.79 -0.84 -0.08]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 6- sum over heads to get pos x d_model attn_out\n",
        "attn_out = np.sum(list(result.values()), axis=0)\n",
        "print_ln()\n",
        "print(f'Attention Out: \\n{printT(attn_out)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCGF4SfUPbhd",
        "outputId": "71f68ae9-7aa8-4704-acc8-8079db480266"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "Attention Out: \n",
            "[[-2.82  3.02  2.63 -3.41  0.65  0.13]\n",
            " [ 1.21  1.03 -1.59 -0.25 -1.61 -1.21]\n",
            " [-0.26  0.97  0.07 -0.07 -0.76 -0.98]\n",
            " [-0.04  0.78  0.43 -0.33  0.37 -0.72]\n",
            " [ 0.89 -0.26  0.26 -1.37  1.03  0.11]\n",
            " [ 1.14 -0.76 -0.18 -0.11  0.7  -0.19]\n",
            " [ 2.08 -0.93 -1.13  0.   -0.06 -0.7 ]\n",
            " [ 0.13 -0.52  0.46 -0.13  0.41 -0.14]\n",
            " [-0.63 -0.46  1.59  0.21  1.45 -0.39]\n",
            " [-0.74 -0.24  1.19 -0.31  0.97  0.12]\n",
            " [-1.06 -0.02  1.69  0.21  0.58 -1.11]\n",
            " [-0.05 -0.84 -0.44  0.34 -0.79  0.19]\n",
            " [-0.97 -0.44  1.5   0.37  0.44 -0.82]\n",
            " [-0.4  -0.25  0.78 -0.31  0.22 -0.25]\n",
            " [-0.18 -0.51  0.6  -0.14  0.48 -0.05]\n",
            " [ 1.21 -1.17 -1.34  0.3  -0.79  0.08]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 7- add back to original residual stream\n",
        "res_stream_tensor = res_stream_tensor + attn_out\n",
        "print_ln()\n",
        "print(f'Updated Residual Stream: \\n{printT(res_stream_tensor)}, {res_stream_tensor.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqgzOv7bPeJ_",
        "outputId": "60d687c9-1ee1-47f9-d6b9-a4aac1a2cf83"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "Updated Residual Stream: \n",
            "[[-3.12  4.33  2.95 -3.21 -0.62  0.42]\n",
            " [ 1.56  3.54 -3.43 -0.28 -0.97 -1.08]\n",
            " [ 1.    2.12 -0.9   0.94 -0.42 -1.2 ]\n",
            " [ 1.22  1.92 -0.54  0.67  0.71 -0.94]\n",
            " [ 0.71  0.14  0.95 -0.63  0.05 -0.18]\n",
            " [ 0.57 -0.43  0.76 -0.33  1.76  1.26]\n",
            " [ 2.61 -0.1   0.09  1.06  0.81 -1.01]\n",
            " [-0.45 -0.18  1.39 -0.35  1.48  1.31]\n",
            " [-1.62 -0.5   0.78 -0.91  1.7  -0.18]\n",
            " [-1.04  1.08  1.51 -0.12 -0.3   0.41]\n",
            " [-0.71  2.49 -0.15  0.18  1.22 -0.99]\n",
            " [ 0.55 -1.47 -0.67  1.08 -0.37  0.46]\n",
            " [-0.63  2.07 -0.34  0.33  1.08 -0.7 ]\n",
            " [-0.97  0.09  1.72 -0.53  1.29  1.2 ]\n",
            " [ 0.43 -1.36  3.09  0.12  1.76 -0.2 ]\n",
            " [ 0.29 -2.01 -1.48  0.61 -1.32  1.43]], (16, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put this together."
      ],
      "metadata": {
        "id": "Z8TkBpWy6JIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_is_all_you_needify(tokens, res_stream_tensor, n_heads=2):\n",
        "  position = len(tokens)\n",
        "  assert position == res_stream_tensor.shape[0]\n",
        "  d_model  = res_stream_tensor.shape[1]\n",
        "  d_head   = int(d_model / n_heads)\n",
        "\n",
        "  query_vec_map = {(tkn, head) : np.random.standard_normal(d_head) for tkn in tokens for head in range(n_heads)}\n",
        "  key_vec_map   = {(tkn, head) : np.random.standard_normal(d_head) for tkn in tokens for head in range(n_heads)}\n",
        "\n",
        "  ### compute attn scores = CausalMask(QK^T/sqrt(d_head)), query_pos x key_pos mat for each head\n",
        "  attn_score_map = {\n",
        "    head : np.zeros((position, position))\n",
        "    for head in range(n_heads)\n",
        "    }\n",
        "\n",
        "  for head in range(n_heads):\n",
        "    for i, dest_tkn in enumerate(tokens):\n",
        "      query = query_vec_map[(dest_tkn, head)]\n",
        "      for j, src_tkn in enumerate(tokens):\n",
        "        key = key_vec_map[(src_tkn, head)]\n",
        "        attn_score = np.dot(query, key)\n",
        "        processed_attn_score = attn_score / np.sqrt(d_head) if j <= i else -1e8\n",
        "      attn_score_map[head][i][j] = processed_attn_score\n",
        "\n",
        "  ## get attn pattern = Softmax(attn_scores)\n",
        "  softmax = lambda vec : np.exp(vec) / np.sum(np.exp(vec))\n",
        "  attn_pattern_map = {\n",
        "      head : np.zeros((position, position))\n",
        "      for head in range(n_heads)\n",
        "      }\n",
        "\n",
        "  for head in range(n_heads):\n",
        "    attn_score_mat = attn_score_map[head]\n",
        "    for i, dest_tkn in enumerate(tokens):\n",
        "      attn_score_vec   = attn_score_mat[i]\n",
        "      attn_pattern_vec = softmax(attn_score_vec)\n",
        "      attn_pattern_map[head][i] = attn_pattern_vec\n",
        "\n",
        "  ### compute z = AttnPattern @ value, shape query_pos x d_head\n",
        "  value_map = {\n",
        "      head : np.random.standard_normal((position, d_head))\n",
        "      for head in range(n_heads)\n",
        "      }\n",
        "  z = {\n",
        "      head : np.matmul(attn_pattern_map[head], value_map[head])\n",
        "      for head in range(n_heads)\n",
        "      }\n",
        "\n",
        "  ### compute result = z @ W_O (d_head, d_model) to get result (pos x d_model)\n",
        "  W_O = {head : np.random.standard_normal((d_head, d_model)) for head in range(n_heads)}\n",
        "  result = {\n",
        "      head : np.matmul(z[head], W_O[head])\n",
        "      for head in range(n_heads)\n",
        "      }\n",
        "  ### sum over heads to get pos x d_model attn_out\n",
        "  attn_out = np.sum(list(result.values()), axis=0)\n",
        "\n",
        "  ### add back to original residual stream\n",
        "  res_stream_tensor = res_stream_tensor + attn_out\n",
        "  return res_stream_tensor\n",
        "\n",
        "input_sequence = 'sup'\n",
        "print(f'Input Sequence: {input_sequence}')\n",
        "\n",
        "res_stream_tensor, tokens = embed(input_sequence)\n",
        "print(f'Tokens: {tokens}')\n",
        "print_ln()\n",
        "print(f'Initial Residual Stream: \\n{printT(res_stream_tensor)}')\n",
        "\n",
        "res_stream_tensor = attention_is_all_you_needify(tokens, res_stream_tensor)\n",
        "print_ln()\n",
        "print(f'Residual Stream post Attn: \\n{printT(res_stream_tensor)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd7ULKI86OoB",
        "outputId": "3decfd4d-7a84-41eb-aed8-72cf2d7cc121"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: sup\n",
            "Tokens: [18, 20, 15, 29]\n",
            "====================\n",
            "Initial Residual Stream: \n",
            "[[-0.34  0.37 -0.12  0.46 -0.5  -0.18]\n",
            " [ 0.78 -0.43  0.41  0.55  0.44 -0.83]\n",
            " [ 1.19  0.89 -1.37  0.35 -1.98  0.05]\n",
            " [-0.92 -0.84 -0.14  0.31 -0.52  1.35]]\n",
            "====================\n",
            "Residual Stream post Attn: \n",
            "[[ 1.34  0.46  0.07 -0.81  1.33 -1.16]\n",
            " [ 2.46 -0.34  0.6  -0.72  2.27 -1.8 ]\n",
            " [ 2.87  0.98 -1.18 -0.92 -0.15 -0.93]\n",
            " [-0.39 -1.45  0.25 -1.16  0.16  2.24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! So we've gone through taking the residual stream and applying attention, enabling us to move information from prior src tokens to the current destination token.\n",
        "\n",
        "We'll now perform some computation on this, using an MLP layer."
      ],
      "metadata": {
        "id": "igPpQykdQzCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3- MLP\n",
        "\n",
        "For each batch x position, we now have a residual stream vector that includes meaningful information about past tokens. We wish to now perform computation on this meaningful residual stream.\n",
        "\n",
        "Specifically, we have for each token a d_model residual stream. Our MLP will first project this vector into a dimension d_mlp = 4 * d_model. We'll then apply a ReLU (should be GeLU but who cares) and then project back down."
      ],
      "metadata": {
        "id": "0Faft6rM5sXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlpify(res_stream_tensor, d_mlp_multiplier=4, verbose=False):\n",
        "  ### res_stream_tensor = position x d_model\n",
        "  d_model = res_stream_tensor.shape[1]\n",
        "  d_mlp   = d_mlp_multiplier * d_model\n",
        "\n",
        "  W1 = np.random.standard_normal((d_model, d_mlp))\n",
        "  linear_out = np.matmul(res_stream_tensor, W1)\n",
        "  ### relu(x) = max(0, x)\n",
        "  relu_out = np.maximum(0, linear_out)\n",
        "\n",
        "  W2 = np.random.standard_normal((d_mlp, d_model))\n",
        "  result = np.matmul(relu_out, W2)\n",
        "  if verbose:\n",
        "    print_ln()\n",
        "    print(f'Res Stream Tensor Input: \\n{printT(res_stream_tensor)}')\n",
        "    print(f'Linear Out: \\n{printT(linear_out)}')\n",
        "    print(f'Post ReLU: \\n{printT(relu_out)}')\n",
        "    print(f'Result: \\n{printT(result)}')\n",
        "  return result\n",
        "\n",
        "res_stream_tensor = mlpify(res_stream_tensor, d_mlp_multiplier=2, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TIGiMw898o6",
        "outputId": "13995cc5-6cc2-416a-964e-52f31af97956"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "Res Stream Tensor Input: \n",
            "[[ 1.34  0.46  0.07 -0.81  1.33 -1.16]\n",
            " [ 2.46 -0.34  0.6  -0.72  2.27 -1.8 ]\n",
            " [ 2.87  0.98 -1.18 -0.92 -0.15 -0.93]\n",
            " [-0.39 -1.45  0.25 -1.16  0.16  2.24]]\n",
            "Linear Out: \n",
            "[[-2.19 -0.7   1.58 -3.76 -0.44  3.09  0.3   4.    1.71 -2.65 -2.4  -0.04]\n",
            " [-0.77  0.96  2.02 -5.22 -2.    7.14  1.94  4.81  1.64 -2.67 -4.18 -0.36]\n",
            " [-7.29  0.82  3.48 -4.32 -1.67  4.75  2.27  3.05  4.25 -6.1   2.25 -2.1 ]\n",
            " [ 8.47  3.17  4.1   0.45  2.87  3.91  5.34 -3.33 -3.98  1.51 -5.8  -1.81]]\n",
            "Post ReLU: \n",
            "[[0.   0.   1.58 0.   0.   3.09 0.3  4.   1.71 0.   0.   0.  ]\n",
            " [0.   0.96 2.02 0.   0.   7.14 1.94 4.81 1.64 0.   0.   0.  ]\n",
            " [0.   0.82 3.48 0.   0.   4.75 2.27 3.05 4.25 0.   2.25 0.  ]\n",
            " [8.47 3.17 4.1  0.45 2.87 3.91 5.34 0.   0.   1.51 0.   0.  ]]\n",
            "Result: \n",
            "[[  2.97  -6.26  -2.17   1.38  -2.56  -6.25]\n",
            " [  6.2   -7.44  -3.37  -0.64   3.25  -6.79]\n",
            " [ 11.22 -11.68   2.64   4.26  -2.76 -13.45]\n",
            " [  4.86  -8.05  -9.7   19.41  16.06  -5.96]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This combination of self attention mechanism followed by a mlp layer is the essence of a *transformer block*."
      ],
      "metadata": {
        "id": "8b6S6KjdMPiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transformer_block(tokens, res_stream_tensor):\n",
        "  attn_out   = attention_is_all_you_needify(tokens, res_stream_tensor)\n",
        "  resid_mid  = attn_out + res_stream_tensor\n",
        "  mlp_out    = mlpify(resid_mid)\n",
        "  resid_post = resid_mid + mlp_out\n",
        "  return resid_post\n",
        "\n",
        "input_sequence = 'sup'\n",
        "res_stream_tensor, tokens = embed(input_sequence)\n",
        "print(f'position = {res_stream_tensor.shape[0]}, d_model = {res_stream_tensor.shape[1]}')\n",
        "output = apply_transformer_block(tokens, res_stream_tensor)\n",
        "print_ln()\n",
        "print(f'Output: \\n{printT(output)}')\n",
        "print(f'Output Shape: {output.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGloMLXbKNOK",
        "outputId": "29d56982-582a-4c64-be65-35ebaa03bedc"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "position = 4, d_model = 6\n",
            "====================\n",
            "Output: \n",
            "[[ 16.87 -17.59  23.44   5.68 -16.58 -25.74]\n",
            " [ 12.89  -3.88  14.11   3.86 -11.4  -27.36]\n",
            " [ 23.3   -2.59  29.    -8.73  -9.69 -36.8 ]\n",
            " [  3.2  -12.66   2.28  -3.33  -6.05  -8.96]]\n",
            "Output Shape: (4, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4- Unembed\n",
        "\n",
        "After applying n_blocks transformer blocks, we presumably have a residual stream that is super meaningful at each token position. From here, we must unembed, i.e. turn into a probability distribution over tokens. That is, given our position x d_model tensor, we will apply a linear map to get to a position x d_vocab tensor. At this point we will have our logits, from which one can apply softmax and have the distribution over tokens for each position."
      ],
      "metadata": {
        "id": "7rJgqnreM_Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_unembed(res_stream_tensor, d_vocab=d_vocab):\n",
        "  W_out = np.random.standard_normal((d_model, d_vocab))\n",
        "  unembed_tensor = np.matmul(res_stream_tensor, W_out)\n",
        "  return unembed_tensor\n",
        "\n",
        "def run_transformer(sequence, n_blocks=2, verbose=False):\n",
        "  res_stream_tensor, tokens = embed(sequence)\n",
        "  if verbose:\n",
        "    print(f\"d_vocab: {d_vocab}\")\n",
        "    print(f\"d_model: {d_model}\")\n",
        "    print(f\"position: {len(tokens)}\")\n",
        "    print(f'n_blocks: {n_blocks}')\n",
        "    print(f'd_mlp: {4 * d_model}')\n",
        "\n",
        "  for i in range(n_blocks):\n",
        "    res_stream_tensor = apply_transformer_block(tokens, res_stream_tensor)\n",
        "\n",
        "  return apply_unembed(res_stream_tensor)\n",
        "\n",
        "input_sequence = 'wow we are done with this transformer implementation'\n",
        "logits = run_transformer(input_sequence, verbose=True)\n",
        "print_ln()\n",
        "print(f'Logits: \\n{printT(logits)}')\n",
        "print(f'Logits Shape: {logits.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKkPIvc9OLVt",
        "outputId": "16943411-7468-4959-9ae3-d92e4514c120"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_vocab: 30\n",
            "d_model: 6\n",
            "position: 53\n",
            "n_blocks: 2\n",
            "d_mlp: 24\n",
            "====================\n",
            "Logits: \n",
            "[[ 106.86   37.81  -48.24 ...  334.94  434.98 -388.64]\n",
            " [-415.22  236.5    44.28 ... -261.52  -90.99  195.33]\n",
            " [ 106.86   37.81  -48.24 ...  334.94  434.98 -388.64]\n",
            " ...\n",
            " [-415.22  236.5    44.28 ... -261.52  -90.99  195.33]\n",
            " [ 218.37    6.04 -128.69 ...  371.32  547.3  -436.89]\n",
            " [-340.05  -45.77  163.73 ... -196.01  -76.     36.5 ]]\n",
            "Logits Shape: (53, 30)\n"
          ]
        }
      ]
    }
  ]
}