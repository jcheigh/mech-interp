{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karpathy Let's Build a GPT Tokenizer \n",
    "\n",
    "This is me roughly following along with [Karpathy's BPE Video](https://www.youtube.com/watch?v=zduSFxRajkE&t=1430s). A cool OpenAI app for visualizing tokenization is [tiktokenizer](https://tiktokenizer.vercel.app/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode/UTF-8\n",
    "\n",
    "At a high level, strings in python are immutable sequences of unicode code points (use ord(char) to find). Encodings can take unicode and convert to bytestrings- UTF8, UTF16, UTF32. \n",
    "\n",
    "Here's more detail, coming from [Nathan Reed's blog](https://www.reedbeta.com/blog/programmers-intro-to-unicode/). Unicode has over 150k characters, \n",
    "as opposed to 256 ASCII. Unicode charactrers are \"code points\", whcih are often written /in hexadecimal with a prefix U+ -> U+0041 or something. The codespace consists of over 1 million code points, with only around 12% actually assigned. \n",
    "\n",
    "Encodings map hexdecimal index in codespace to bytes. The easiest way is to just store it as a 32-bit integer, which consumes 4 bytes per code point. This is lot of extra storage; UTF-32 = Unicode Transformation Format is a bit better but still a lot. More common are UTF-8/UTF-16, which are \"variable length encodings\"- 8 bit/16 bit units. Here, code points with smaller index values take up fewer bytes, but it is a bit slower because more involved. \n",
    "\n",
    "UTF-8: code points are stored in 1-4 bytes, based on index value. For example, the 128 ASCII characters are encoded as single bytes, so ASCII files can be interpreted as UTF-8 without conversion. \n",
    "\n",
    "Unicode also enables things like composing marks (combining two characters into one, canonical equivalence- equating equivalent characters, normalization forms- equivalence class of characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58, 41, 32, 116, 111, 32, 116, 101, 115, 116, 32, 117, 116, 102, 56]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\":) to test utf8\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255, 254, 117, 0, 116, 0, 102, 0, 49, 0, 54, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"utf16\".encode(\"utf-16\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note lots of \"wasteful\" encoding to 0s. We generally want to use UTF-8 and then maybe further combine, i.e. enable us to change |V| as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding\n",
    "\n",
    "* input = aaabdaaabac\n",
    "* vocab = {a, b, c, d}\n",
    "* vocab_size = |vocab|\n",
    "* while vocab_size > ideal_size:\n",
    "    * Let z := most frequent pair (z = aa)\n",
    "    * replace(z, aa) (seq = zabdzabac, vocab = {z,a,b,c,d}\n",
    "* return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Bytes: b'\\xef\\xbc\\xb5\\xef\\xbd'\n",
      "Text: ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "Len(text): 533\n",
      "====================\n",
      "Tokens: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Len(tokens): 616\n"
     ]
    }
   ],
   "source": [
    "text = \"ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode('utf-8') \n",
    "print(f'Raw Bytes: {tokens[:5]}')\n",
    "tokens = list(map(int, tokens))\n",
    "print(f'Text: {text}') \n",
    "print(f'Len(text): {len(text)}')\n",
    "print(f'=' *20)\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'Len(tokens): {len(tokens)}')\n",
    "### each char is 1-4 bytes so 533 text -> 616 tokens makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Pair: (101, 32), Chars: ('e', ' ')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def find_most_common_pair(tokens):\n",
    "    return Counter(zip(tokens, tokens[1:])).most_common(1)[0][0]\n",
    "\n",
    "top_pair = find_most_common_pair(tokens)\n",
    "print(f'Most Common Pair: {top_pair}, Chars: {chr(top_pair[0]), chr(top_pair[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 5, 1, 2, 104]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    ### list of ints (ids), replace(pair, idx)\n",
    "    newids = []\n",
    "    i = 0 \n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i+=1 \n",
    "    return newids\n",
    "\n",
    "print(merge([5,4,5,1,2,5,4], (5,4), 104))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into token 256\n",
      "merging (240, 159) into token 257\n",
      "merging (226, 128) into token 258\n",
      "merging (105, 110) into token 259\n",
      "merging (115, 32) into token 260\n",
      "merging (97, 110) into token 261\n",
      "merging (116, 104) into token 262\n",
      "merging (257, 133) into token 263\n",
      "merging (257, 135) into token 264\n",
      "merging (97, 114) into token 265\n",
      "merging (239, 189) into token 266\n",
      "merging (258, 140) into token 267\n",
      "merging (267, 264) into token 268\n",
      "merging (101, 114) into token 269\n",
      "merging (111, 114) into token 270\n",
      "merging (116, 32) into token 271\n",
      "merging (259, 103) into token 272\n",
      "merging (115, 116) into token 273\n",
      "merging (261, 100) into token 274\n"
     ]
    }
   ],
   "source": [
    "ideal_size = 275\n",
    "num_merges = ideal_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    pair = find_most_common_pair(ids)\n",
    "    idx = 256 + i \n",
    "    print(f'merging {pair} into token {idx}')\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
